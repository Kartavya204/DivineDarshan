# AI Service Configuration
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Optional: Local LLM Configuration
LOCAL_LLM_URL=http://localhost:1234/v1
LOCAL_LLM_API_KEY=your_local_llm_key

# AI Service Preferences
PREFERRED_AI_PROVIDER=openai # Options: openai, anthropic, openrouter, local
FALLBACK_AI_PROVIDER=anthropic

# Rate Limiting
AI_MAX_REQUESTS_PER_MINUTE=60
AI_REQUEST_TIMEOUT=30000

# Caching
AI_CACHE_ENABLED=true
AI_CACHE_TTL=300 # 5 minutes

# Server Configuration
PORT=5001
NODE_ENV=development

# Redis (Optional - for distributed caching)
REDIS_URL=redis://localhost:6379

# Monitoring
LOG_LEVEL=info
